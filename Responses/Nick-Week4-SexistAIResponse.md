While reading the [Bloomberg](https://www.bloomberg.com/technology) article, a couple of lines struck me in terms of the connversation of Biased AI. First, the article stated that, "AI is only as good as the data it learns from". This line is very important in the understanding of how biased AI can come about. My first reaction is that the biased comes from the human's that programmed the AI itself. Later in the article they reiterate this idea by saying, "In some cases the algorithms are trained to learn from the people using the software and, over time, pick up the biases of  the human users." While I tend to agree with this statement, problems also arise if there isnt enough data to train on. Maybe the data they are using just isnt diverse enough. With a lack of diversity, problems of biasness with come up. 

Meanwhile, the [Guardian](https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals) article reclaims some of these same ideas. For example, Joanna Bryson says, “A lot of people are saying this is showing that AI is prejudiced. No. This is showing we’re prejudiced and that AI is learning it.” Once again, this calls back to how the humans training the AI are the ones leading it towards biasness. It reminds me of the Twitter Bot that Microsoft put out a couple of years ago. Infamously, it had to be taken down after becoming racist towards other Twitter users, and claiming it was a Nazi. The reason the bot started to act this way though, was due to users trolling the bot. The humans on Twitter were trying to make it become something with prejudice. It goes to show that AI are not made with biases on purpose, and they wont learn to discriminate against a group of people by their own accord. If, however, the humans that train the AI have biases or aren't careful about having a diverse training set, then AI will tend to the flaws of biasness. 
