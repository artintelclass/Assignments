As machines become more and more capable of decision making, they start to enter the human world where they have to take responsibility for lives. The human world isn’t as simple and calculatable as the binary code and numbers. Driverless cars in particular created a wave of questions that engineers need to answer about the ethics of decision making. Can we use obscure machine learning to make crucial decisions or should only rule based approaches supported to always have a clear answer to “why”? How should a driverless car decide in a dilemma , where it has to choose in accident to kill either high value doctors or criminals, elderly or young, people breaking the traffic law? One question that particularly stuck with me is whether it should kill the passengers if it knows they are invaluable, unmotivated people, but an athlete stepped down the curb? The other question is how would the general public react if we gave such moral ethics to machines? Could they accept the calculated risks a machine takes into account, or would they stick to the sometimes irrational human instinct in such situations? Regardless, I do believe that a well engineered system would be able to make decisions then humans.
