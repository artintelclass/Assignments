{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_184 (Dense)            (None, 50)                700       \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 3,352\n",
      "Trainable params: 3,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 290 samples, validate on 13 samples\n",
      "Epoch 1/100\n",
      "290/290 [==============================] - 8s 26ms/step - loss: 6.9500 - acc: 0.5517 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 2/100\n",
      "290/290 [==============================] - 0s 817us/step - loss: 6.0877 - acc: 0.5759 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 3/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 6.6864 - acc: 0.5345 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 4/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 6.7480 - acc: 0.5483 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 5/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 6.6005 - acc: 0.5655 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 6/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 6.1023 - acc: 0.5517 - val_loss: 9.4110 - val_acc: 0.3077\n",
      "Epoch 7/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 6.0839 - acc: 0.5690 - val_loss: 3.2650 - val_acc: 0.5385\n",
      "Epoch 8/100\n",
      "290/290 [==============================] - 0s 947us/step - loss: 6.7385 - acc: 0.5276 - val_loss: 1.4445 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "290/290 [==============================] - 0s 866us/step - loss: 6.1687 - acc: 0.5345 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 10/100\n",
      "290/290 [==============================] - 0s 872us/step - loss: 6.0924 - acc: 0.5655 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 11/100\n",
      "290/290 [==============================] - 0s 870us/step - loss: 6.2655 - acc: 0.5724 - val_loss: 12.3985 - val_acc: 0.2308\n",
      "Epoch 12/100\n",
      "290/290 [==============================] - 0s 875us/step - loss: 5.8436 - acc: 0.5655 - val_loss: 10.6151 - val_acc: 0.2308\n",
      "Epoch 13/100\n",
      "290/290 [==============================] - 0s 840us/step - loss: 5.9338 - acc: 0.5690 - val_loss: 11.7703 - val_acc: 0.2308\n",
      "Epoch 14/100\n",
      "290/290 [==============================] - 0s 924us/step - loss: 6.1264 - acc: 0.5483 - val_loss: 5.7793 - val_acc: 0.3077\n",
      "Epoch 15/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 5.7415 - acc: 0.5586 - val_loss: 10.8764 - val_acc: 0.1538\n",
      "Epoch 16/100\n",
      "290/290 [==============================] - 0s 861us/step - loss: 5.7671 - acc: 0.5759 - val_loss: 10.7991 - val_acc: 0.2308\n",
      "Epoch 17/100\n",
      "290/290 [==============================] - 0s 964us/step - loss: 5.8481 - acc: 0.5793 - val_loss: 5.7657 - val_acc: 0.3846\n",
      "Epoch 18/100\n",
      "290/290 [==============================] - 0s 897us/step - loss: 5.8505 - acc: 0.5759 - val_loss: 12.1761 - val_acc: 0.2308\n",
      "Epoch 19/100\n",
      "290/290 [==============================] - 0s 878us/step - loss: 5.3726 - acc: 0.5931 - val_loss: 12.3360 - val_acc: 0.2308\n",
      "Epoch 20/100\n",
      "290/290 [==============================] - 0s 939us/step - loss: 5.5810 - acc: 0.5862 - val_loss: 10.8275 - val_acc: 0.2308\n",
      "Epoch 21/100\n",
      "290/290 [==============================] - 0s 934us/step - loss: 6.4148 - acc: 0.5448 - val_loss: 10.5669 - val_acc: 0.2308\n",
      "Epoch 22/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 5.6559 - acc: 0.5828 - val_loss: 12.1011 - val_acc: 0.2308\n",
      "Epoch 23/100\n",
      "290/290 [==============================] - 0s 962us/step - loss: 5.6842 - acc: 0.5655 - val_loss: 10.4526 - val_acc: 0.2308\n",
      "Epoch 24/100\n",
      "290/290 [==============================] - 0s 834us/step - loss: 5.4129 - acc: 0.5724 - val_loss: 8.3382 - val_acc: 0.3846\n",
      "Epoch 25/100\n",
      "290/290 [==============================] - 0s 866us/step - loss: 5.3077 - acc: 0.5897 - val_loss: 9.1189 - val_acc: 0.2308\n",
      "Epoch 26/100\n",
      "290/290 [==============================] - 0s 847us/step - loss: 4.9124 - acc: 0.6000 - val_loss: 4.7339 - val_acc: 0.3846\n",
      "Epoch 27/100\n",
      "290/290 [==============================] - 0s 838us/step - loss: 5.5531 - acc: 0.5931 - val_loss: 12.1126 - val_acc: 0.2308\n",
      "Epoch 28/100\n",
      "290/290 [==============================] - 0s 869us/step - loss: 4.6088 - acc: 0.6310 - val_loss: 3.1046 - val_acc: 0.5385\n",
      "Epoch 29/100\n",
      "290/290 [==============================] - 0s 917us/step - loss: 4.8696 - acc: 0.5931 - val_loss: 6.4864 - val_acc: 0.3077\n",
      "Epoch 30/100\n",
      "290/290 [==============================] - 0s 751us/step - loss: 4.1288 - acc: 0.6069 - val_loss: 3.4624 - val_acc: 0.3846\n",
      "Epoch 31/100\n",
      "290/290 [==============================] - 0s 766us/step - loss: 4.0527 - acc: 0.6069 - val_loss: 2.6522 - val_acc: 0.4615\n",
      "Epoch 32/100\n",
      "290/290 [==============================] - 0s 908us/step - loss: 3.9834 - acc: 0.6138 - val_loss: 3.3403 - val_acc: 0.3846\n",
      "Epoch 33/100\n",
      "290/290 [==============================] - 0s 781us/step - loss: 4.0112 - acc: 0.6000 - val_loss: 1.1697 - val_acc: 0.7692\n",
      "Epoch 34/100\n",
      "290/290 [==============================] - 0s 833us/step - loss: 3.4359 - acc: 0.6448 - val_loss: 1.1506 - val_acc: 0.7692\n",
      "Epoch 35/100\n",
      "290/290 [==============================] - 0s 843us/step - loss: 2.7177 - acc: 0.6897 - val_loss: 1.4262 - val_acc: 0.4615\n",
      "Epoch 36/100\n",
      "290/290 [==============================] - 0s 794us/step - loss: 2.9018 - acc: 0.6310 - val_loss: 0.6108 - val_acc: 0.8462\n",
      "Epoch 37/100\n",
      "290/290 [==============================] - 0s 911us/step - loss: 1.7708 - acc: 0.6724 - val_loss: 0.4924 - val_acc: 0.8462\n",
      "Epoch 38/100\n",
      "290/290 [==============================] - 0s 866us/step - loss: 1.8086 - acc: 0.6586 - val_loss: 1.0360 - val_acc: 0.3846\n",
      "Epoch 39/100\n",
      "290/290 [==============================] - 0s 779us/step - loss: 1.5464 - acc: 0.6103 - val_loss: 0.9821 - val_acc: 0.2308\n",
      "Epoch 40/100\n",
      "290/290 [==============================] - 0s 789us/step - loss: 1.3045 - acc: 0.6310 - val_loss: 0.7371 - val_acc: 0.3077\n",
      "Epoch 41/100\n",
      "290/290 [==============================] - 0s 802us/step - loss: 1.2826 - acc: 0.6000 - val_loss: 0.6532 - val_acc: 0.6154\n",
      "Epoch 42/100\n",
      "290/290 [==============================] - 0s 895us/step - loss: 1.1780 - acc: 0.6138 - val_loss: 0.8831 - val_acc: 0.2308\n",
      "Epoch 43/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.7592 - acc: 0.6310 - val_loss: 0.6158 - val_acc: 0.7692\n",
      "Epoch 44/100\n",
      "290/290 [==============================] - 0s 904us/step - loss: 0.9681 - acc: 0.6138 - val_loss: 0.6932 - val_acc: 0.3077\n",
      "Epoch 45/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.8458 - acc: 0.6517 - val_loss: 0.6932 - val_acc: 0.3846\n",
      "Epoch 46/100\n",
      "290/290 [==============================] - 0s 899us/step - loss: 0.8612 - acc: 0.5828 - val_loss: 0.6255 - val_acc: 0.7692\n",
      "Epoch 47/100\n",
      "290/290 [==============================] - 0s 897us/step - loss: 0.7898 - acc: 0.5862 - val_loss: 0.6819 - val_acc: 0.3077\n",
      "Epoch 48/100\n",
      "290/290 [==============================] - 0s 841us/step - loss: 0.7305 - acc: 0.6172 - val_loss: 0.6386 - val_acc: 0.8462\n",
      "Epoch 49/100\n",
      "290/290 [==============================] - 0s 800us/step - loss: 0.7174 - acc: 0.6448 - val_loss: 0.6819 - val_acc: 0.3846\n",
      "Epoch 50/100\n",
      "290/290 [==============================] - 0s 833us/step - loss: 0.6672 - acc: 0.6483 - val_loss: 0.6834 - val_acc: 0.3846\n",
      "Epoch 51/100\n",
      "290/290 [==============================] - 0s 793us/step - loss: 0.7162 - acc: 0.6552 - val_loss: 0.7105 - val_acc: 0.3846\n",
      "Epoch 52/100\n",
      "290/290 [==============================] - 0s 869us/step - loss: 0.6820 - acc: 0.6414 - val_loss: 0.6821 - val_acc: 0.3846\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 0s 902us/step - loss: 0.6581 - acc: 0.6172 - val_loss: 0.6224 - val_acc: 0.6923\n",
      "Epoch 54/100\n",
      "290/290 [==============================] - 0s 818us/step - loss: 0.7586 - acc: 0.6310 - val_loss: 0.6390 - val_acc: 0.5385\n",
      "Epoch 55/100\n",
      "290/290 [==============================] - 0s 798us/step - loss: 0.6139 - acc: 0.6897 - val_loss: 0.6649 - val_acc: 0.3846\n",
      "Epoch 56/100\n",
      "290/290 [==============================] - 0s 839us/step - loss: 0.6342 - acc: 0.6483 - val_loss: 0.5979 - val_acc: 0.6923\n",
      "Epoch 57/100\n",
      "290/290 [==============================] - 0s 877us/step - loss: 0.7767 - acc: 0.6655 - val_loss: 0.6015 - val_acc: 0.6923\n",
      "Epoch 58/100\n",
      "290/290 [==============================] - 0s 821us/step - loss: 0.6474 - acc: 0.6966 - val_loss: 0.6608 - val_acc: 0.3846\n",
      "Epoch 59/100\n",
      "290/290 [==============================] - 0s 746us/step - loss: 0.5894 - acc: 0.7000 - val_loss: 0.5963 - val_acc: 0.8462\n",
      "Epoch 60/100\n",
      "290/290 [==============================] - 0s 859us/step - loss: 0.6275 - acc: 0.6862 - val_loss: 0.6339 - val_acc: 0.4615\n",
      "Epoch 61/100\n",
      "290/290 [==============================] - 0s 823us/step - loss: 0.6380 - acc: 0.6517 - val_loss: 0.5842 - val_acc: 0.7692\n",
      "Epoch 62/100\n",
      "290/290 [==============================] - 0s 921us/step - loss: 0.6409 - acc: 0.6414 - val_loss: 0.6054 - val_acc: 0.6154\n",
      "Epoch 63/100\n",
      "290/290 [==============================] - 0s 864us/step - loss: 0.5940 - acc: 0.6862 - val_loss: 0.6152 - val_acc: 0.6154\n",
      "Epoch 64/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.5735 - acc: 0.6897 - val_loss: 0.5981 - val_acc: 0.6923\n",
      "Epoch 65/100\n",
      "290/290 [==============================] - 0s 920us/step - loss: 0.5902 - acc: 0.7138 - val_loss: 0.5810 - val_acc: 0.6923\n",
      "Epoch 66/100\n",
      "290/290 [==============================] - 0s 884us/step - loss: 0.6366 - acc: 0.6793 - val_loss: 0.5816 - val_acc: 0.6923\n",
      "Epoch 67/100\n",
      "290/290 [==============================] - 0s 829us/step - loss: 0.6185 - acc: 0.6931 - val_loss: 0.6435 - val_acc: 0.3846\n",
      "Epoch 68/100\n",
      "290/290 [==============================] - 0s 881us/step - loss: 0.5612 - acc: 0.6862 - val_loss: 0.5679 - val_acc: 0.7692\n",
      "Epoch 69/100\n",
      "290/290 [==============================] - 0s 799us/step - loss: 0.6169 - acc: 0.6897 - val_loss: 0.6209 - val_acc: 0.5385\n",
      "Epoch 70/100\n",
      "290/290 [==============================] - 0s 810us/step - loss: 0.5525 - acc: 0.7379 - val_loss: 0.6100 - val_acc: 0.5385\n",
      "Epoch 71/100\n",
      "290/290 [==============================] - 0s 807us/step - loss: 0.5805 - acc: 0.7172 - val_loss: 0.5759 - val_acc: 0.6154\n",
      "Epoch 72/100\n",
      "290/290 [==============================] - 0s 803us/step - loss: 0.5874 - acc: 0.7310 - val_loss: 0.5662 - val_acc: 0.7692\n",
      "Epoch 73/100\n",
      "290/290 [==============================] - 0s 801us/step - loss: 0.5402 - acc: 0.7034 - val_loss: 0.5268 - val_acc: 0.8462\n",
      "Epoch 74/100\n",
      "290/290 [==============================] - 0s 797us/step - loss: 0.5614 - acc: 0.7103 - val_loss: 0.5305 - val_acc: 0.6923\n",
      "Epoch 75/100\n",
      "290/290 [==============================] - 0s 783us/step - loss: 0.5773 - acc: 0.7241 - val_loss: 0.6011 - val_acc: 0.5385\n",
      "Epoch 76/100\n",
      "290/290 [==============================] - 0s 766us/step - loss: 0.5370 - acc: 0.7138 - val_loss: 0.6101 - val_acc: 0.5385\n",
      "Epoch 77/100\n",
      "290/290 [==============================] - 0s 817us/step - loss: 0.5521 - acc: 0.7241 - val_loss: 0.6035 - val_acc: 0.4615\n",
      "Epoch 78/100\n",
      "290/290 [==============================] - 0s 847us/step - loss: 0.6264 - acc: 0.7207 - val_loss: 0.5704 - val_acc: 0.5385\n",
      "Epoch 79/100\n",
      "290/290 [==============================] - 0s 745us/step - loss: 0.5599 - acc: 0.7345 - val_loss: 0.5609 - val_acc: 0.5385\n",
      "Epoch 80/100\n",
      "290/290 [==============================] - 0s 845us/step - loss: 0.5405 - acc: 0.7483 - val_loss: 0.5491 - val_acc: 0.6154\n",
      "Epoch 81/100\n",
      "290/290 [==============================] - 0s 907us/step - loss: 0.5919 - acc: 0.7103 - val_loss: 0.5946 - val_acc: 0.6154\n",
      "Epoch 82/100\n",
      "290/290 [==============================] - 0s 847us/step - loss: 0.5846 - acc: 0.6897 - val_loss: 0.5103 - val_acc: 0.7692\n",
      "Epoch 83/100\n",
      "290/290 [==============================] - 0s 858us/step - loss: 0.5431 - acc: 0.7414 - val_loss: 0.5421 - val_acc: 0.6154\n",
      "Epoch 84/100\n",
      "290/290 [==============================] - 0s 895us/step - loss: 0.5297 - acc: 0.7379 - val_loss: 0.5524 - val_acc: 0.6154\n",
      "Epoch 85/100\n",
      "290/290 [==============================] - 0s 978us/step - loss: 0.5552 - acc: 0.7172 - val_loss: 0.5594 - val_acc: 0.6154\n",
      "Epoch 86/100\n",
      "290/290 [==============================] - 0s 997us/step - loss: 0.5711 - acc: 0.7241 - val_loss: 0.5480 - val_acc: 0.6154\n",
      "Epoch 87/100\n",
      "290/290 [==============================] - 0s 944us/step - loss: 0.5429 - acc: 0.7276 - val_loss: 0.5118 - val_acc: 0.7692\n",
      "Epoch 88/100\n",
      "290/290 [==============================] - 0s 874us/step - loss: 0.5315 - acc: 0.7345 - val_loss: 0.5226 - val_acc: 0.7692\n",
      "Epoch 89/100\n",
      "290/290 [==============================] - 0s 909us/step - loss: 0.5607 - acc: 0.7172 - val_loss: 0.4579 - val_acc: 0.9231\n",
      "Epoch 90/100\n",
      "290/290 [==============================] - 0s 988us/step - loss: 0.5244 - acc: 0.7414 - val_loss: 0.4998 - val_acc: 0.8462\n",
      "Epoch 91/100\n",
      "290/290 [==============================] - 0s 837us/step - loss: 0.5516 - acc: 0.7414 - val_loss: 0.5763 - val_acc: 0.6154\n",
      "Epoch 92/100\n",
      "290/290 [==============================] - 0s 826us/step - loss: 0.5444 - acc: 0.7414 - val_loss: 0.5159 - val_acc: 0.6923\n",
      "Epoch 93/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.5318 - acc: 0.7483 - val_loss: 0.5316 - val_acc: 0.6923\n",
      "Epoch 94/100\n",
      "290/290 [==============================] - 0s 809us/step - loss: 0.4568 - acc: 0.8069 - val_loss: 0.4930 - val_acc: 0.8462\n",
      "Epoch 95/100\n",
      "290/290 [==============================] - 0s 731us/step - loss: 0.4877 - acc: 0.7483 - val_loss: 0.5592 - val_acc: 0.7692\n",
      "Epoch 96/100\n",
      "290/290 [==============================] - 0s 974us/step - loss: 0.5504 - acc: 0.7345 - val_loss: 0.5829 - val_acc: 0.6154\n",
      "Epoch 97/100\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.5375 - acc: 0.7552 - val_loss: 0.5441 - val_acc: 0.6923\n",
      "Epoch 98/100\n",
      "290/290 [==============================] - 0s 998us/step - loss: 0.4922 - acc: 0.7517 - val_loss: 0.5383 - val_acc: 0.8462\n",
      "Epoch 99/100\n",
      "290/290 [==============================] - 0s 999us/step - loss: 0.4426 - acc: 0.7897 - val_loss: 0.5304 - val_acc: 0.7692\n",
      "Epoch 100/100\n",
      "290/290 [==============================] - 0s 909us/step - loss: 0.5630 - acc: 0.7483 - val_loss: 0.4828 - val_acc: 0.8462\n",
      "Test loss: 0.482837796211\n",
      "Test accuracy: 0.846153855324\n",
      "Max score so far is 0.846153855324\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.contrib.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We don't have a header, and we don't want the first column to be index values\n",
    "training = pd.read_csv(\"../mnist_dataset/cleveland_data_train.csv\", header=None, index_col=False)\n",
    "# Take the first column and store as labels as numpy array\n",
    "training_labels = np.asfarray(training.iloc[:,13])\n",
    "training_labels= np.clip(training_labels,0,1)\n",
    "# Delete the first column and use the rest as data as numpy array\n",
    "x_train = np.asfarray(training.drop(columns=[13]))\n",
    "# We don't have a header, and we don't want the first column to be index values\n",
    "test = pd.read_csv(\"../mnist_dataset/cleveland_data_test.csv\", header=None, index_col=False)\n",
    "# Take the first column and store as labels as numpy array\n",
    "test_labels = np.asfarray(test.iloc[:,13])\n",
    "test_labels= np.clip(test_labels,0,1)\n",
    "# Delete the first column and use the rest as data as numpy array\n",
    "x_test = np.asfarray(test.drop(columns=[13]))\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(training_labels, num_classes)\n",
    "y_test = keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "num_input_nodes = 13\n",
    "num_hidden_nodes = 50\n",
    "\n",
    "# Counters for successive loops\n",
    "scoreCounter = 0\n",
    "iterations = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    print(\"Iteration number: \" + str(iterations))\n",
    "    model = Sequential() # means we have layers that are stacked on each other in sequence\n",
    "    model.add(Dense(num_hidden_nodes, activation='relu', input_shape=(13,)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_hidden_nodes, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary() # prints out a representation of the model\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=False)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    iterations += 1\n",
    "    \n",
    "    if score [1] > scoreCounter:\n",
    "        scoreCounter = score [1]\n",
    "        \n",
    "    print(\"Max score so far is \" + str(scoreCounter))\n",
    "    \n",
    "    if score [1] >= 0.8:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
